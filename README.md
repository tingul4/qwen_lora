# ğŸš— Qwen2-VL LoRA è»Šç¦é æ¸¬æ¨¡å‹ - ç’°å¢ƒå»ºç½®æŒ‡å—

æœ¬å°ˆæ¡ˆä½¿ç”¨ **Python 3.10**ï¼Œä¸¦æ¡ç”¨ [uv](https://github.com/astral-sh/uv) é€²è¡Œæ¥µé€Ÿçš„å¥—ä»¶ç®¡ç†èˆ‡è™›æ“¬ç’°å¢ƒé…ç½®ã€‚

## ğŸ“‹ å‰ç½®éœ€æ±‚ (Prerequisites)

  * **OS**: Linux (æ¨è–¦ Ubuntu 20.04/22.04) æˆ– Windows (WSL2)
  * **GPU**: NVIDIA é¡¯å¡ (å»ºè­° VRAM \>= 24GB)ï¼Œä¸¦å·²å®‰è£é©…å‹•ç¨‹å¼ã€‚
  * **CUDA**: å»ºè­° CUDA 12.1 æˆ–ä»¥ä¸Šç‰ˆæœ¬ã€‚

-----

## ğŸš€ å¿«é€Ÿé–‹å§‹ (Quick Start)

### 1\. å®‰è£ uv (å¦‚æœå°šæœªå®‰è£)

**Linux / macOS:**

```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
```

**Windows:**

```powershell
powershell -c "irm https://astral.sh/uv/install.ps1 | iex"
```

### 2\. åˆå§‹åŒ–å°ˆæ¡ˆç’°å¢ƒ

è«‹ä¾åºåŸ·è¡Œä»¥ä¸‹æŒ‡ä»¤ï¼š

```bash
# 1. å»ºç«‹ Python 3.10 è™›æ“¬ç’°å¢ƒ (ç³»çµ±æœƒè‡ªå‹•ä¸‹è¼‰ managed python å¦‚æœæ²’å®‰è£)
uv venv --python 3.10 .venv

# 2. å•Ÿå‹•è™›æ“¬ç’°å¢ƒ
# Linux / macOS:
source .venv/bin/activate
# Windows:
# .venv\Scripts\activate

# 3. å»ºç«‹ requirements.txt (è¤‡è£½ä¸‹æ–¹å…§å®¹)
# (è«‹åƒè¦‹ä¸‹æ–¹ "ä¾è³´åˆ—è¡¨" ç« ç¯€)
```

### 3\. ä¾è³´åˆ—è¡¨ (requirements.txt)

è«‹åœ¨å°ˆæ¡ˆæ ¹ç›®éŒ„å»ºç«‹ `requirements.txt`ï¼Œå…§å®¹å¦‚ä¸‹ï¼š

```text
# --- Transformers & LoRA ---
transformers>=4.45.0
accelerate>=0.34.0
huggingface-hub
peft>=0.12.0
bitsandbytes>=0.43.3
qwen-vl-utils>=0.0.8

# --- Utilities ---
numpy<2.0.0
pandas
scikit-learn
pillow
tqdm
tensorboard

# --- Low-level Dependencies ---
pytz
python-dateutil
six
typing_extensions
requests
packaging
pyyaml
```

### 4\. å®‰è£å¥—ä»¶

ä½¿ç”¨ `uv` é€²è¡Œæ¥µé€Ÿå®‰è£ï¼š

```bash
# æ­¥é©Ÿ A: å®‰è£ PyTorch (æŒ‡å®šå®˜æ–¹ CU121 å€‰åº«)
uv pip install torch==2.4.1 torchvision==0.19.1 --index-url https://download.pytorch.org/whl/cu121

# æ­¥é©Ÿ B: å®‰è£ä¸»è¦ä¾è³´
uv pip install -r requirements.txt

# æ­¥é©Ÿ C: å®‰è£ Flash Attention 2 (å¼·çƒˆå»ºè­°ï¼ŒåŠ é€Ÿè¨“ç·´èˆ‡ç¯€çœé¡¯å­˜)
# æ³¨æ„ï¼šé€™ä¸€æ­¥éœ€è¦ç³»çµ±æœ‰å®‰è£ CUDA Toolkit (nvcc)
uv pip install flash-attn --no-build-isolation
```

> **âš ï¸ Flash Attention å®‰è£å¤±æ•—æ€éº¼è¾¦ï¼Ÿ**
> å¦‚æœæ­¥é©Ÿ B å ±éŒ¯ï¼Œé€šå¸¸æ˜¯å› ç‚ºç·¨è­¯ç’°å¢ƒå•é¡Œã€‚ä½ å¯ä»¥å˜—è©¦ä¸‹è¼‰é ç·¨è­¯å¥½çš„ wheel æª”å®‰è£ (ä»¥ Linux, Python 3.10, Torch 2.4, CUDA 12.x ç‚ºä¾‹)ï¼š
>
> ```bash
> uv pip install https://github.com/Dao-AILab/flash-attention/releases/download/v2.6.3/flash_attn-2.6.3+cu123torch2.4cxx11abiFALSE-cp310-cp310-linux_x86_64.whl --no-build-isolation
> ```

-----

## âœ… é©—è­‰ç’°å¢ƒ

å»ºç«‹ä¸€å€‹ `check_env.py` æª”æ¡ˆä¸¦åŸ·è¡Œï¼Œç¢ºèªç’°å¢ƒæ˜¯å¦å°±ç·’ï¼š

```python
import torch
import sys
try:
    import flash_attn
    fa_status = "Installed âœ…"
except ImportError:
    fa_status = "Not Found âš ï¸ (Training will be slower)"

print(f"=======================================")
print(f"Python Version: {sys.version.split()[0]}")
print(f"Torch Version: {torch.__version__}")
print(f"CUDA Available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"CUDA Version: {torch.version.cuda}")
    print(f"GPU: {torch.cuda.get_device_name(0)}")
    print(f"BF16 Support: {torch.cuda.is_bf16_supported()}")
print(f"Flash Attention: {fa_status}")
print(f"=======================================")
```

åŸ·è¡Œæª¢æŸ¥ï¼š

```bash
python check_env.py
```

-----

## ğŸ“‚ å»ºè­°çš„å°ˆæ¡ˆçµæ§‹

ç‚ºäº†é…åˆè¨“ç·´ç¨‹å¼ç¢¼ï¼Œå»ºè­°ä½ çš„å°ˆæ¡ˆç›®éŒ„çµæ§‹å¦‚ä¸‹ï¼š

```text
project_root/
â”œâ”€â”€ .venv/                 # uv å»ºç«‹çš„è™›æ“¬ç’°å¢ƒ
â”œâ”€â”€ requirements.txt       # å¥—ä»¶æ¸…å–®
â”œâ”€â”€ README.md              # æœ¬èªªæ˜æª”
â”œâ”€â”€ train_lora.py          # ä¸»è¦è¨“ç·´ç¨‹å¼ç¢¼
â”œâ”€â”€ output.csv             # è¨“ç·´è³‡æ–™ç´¢å¼• (Train)
â”œâ”€â”€ gt_public.csv          # é©—è­‰è³‡æ–™ç´¢å¼• (Val)
â””â”€â”€ dataset/               # (å»ºè­°) é€é Symbolic Link é€£çµåˆ°æœ¬ç›®éŒ„
```